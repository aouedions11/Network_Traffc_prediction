{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import time\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, Bidirectional, GRU\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import seaborn as sns\n",
    "from numpy import loadtxt\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from keras.models import load_model\n",
    "from time import sleep\n",
    "from tqdm import tqdm, trange\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_path = ('testbed_flat_tms_up.csv')\n",
    "dataset = pd.read_csv(dataset_path, parse_dates=[\"Date\"])\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset=dataset.set_index(['Date'])\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_steps = dataset.shape[0]\n",
    "train_size = int(total_steps * 0.7)\n",
    "val_size = int(total_steps * 0.1)\n",
    "\n",
    "train_df, val_df, test_df = dataset[0:train_size], dataset[train_size:train_size + val_size], \\\n",
    "                                dataset[train_size + val_size: ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=pd.DataFrame(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df=pd.DataFrame(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df=pd.DataFrame(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = MinMaxScaler(feature_range=(0,1))\n",
    "sc=sc.fit(train_df)\n",
    "train_df = sc.transform(train_df)\n",
    "test_df = sc.transform(test_df)\n",
    "val_df = sc.transform(val_df)\n",
    "#print(\"train_df\", train_df)\n",
    "#print(\"test_df\", test_df)\n",
    "train_df=pd.DataFrame(train_df) \n",
    "test_df=pd.DataFrame(test_df)\n",
    "val_df=pd.DataFrame(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(X, y , seq_len=1):\n",
    "    Xs, ys = [], []\n",
    "    for i in tqdm (range(len(X)-seq_len)):\n",
    "        v=X.iloc[i:(i+seq_len)].to_numpy()\n",
    "        Xs.append(v)\n",
    "        ys.append(y.iloc[i+seq_len])\n",
    "    return np.array(Xs), np.array(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_TM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, seq_len):\n",
    "        super(LSTM_TM, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        #self.n_layer=n_layer\n",
    "        self.seq_len = seq_len\n",
    "        #self.pred_len = pred_len\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size= hidden_dim,\n",
    "            batch_first = True,\n",
    "            #bidirectional=True,\n",
    "            dropout=0.2)\n",
    "        self.fc = nn.Linear(hidden_dim, input_dim)\n",
    "        #self.time_linear = nn.Linear(seq_len, pred_len)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc(x)\n",
    "        y_pred = x[:,-1]\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1e-8\n",
    "\n",
    "\n",
    "def rse(preds, labels):\n",
    "    return torch.sum((preds - labels) ** 2) / torch.sum((labels + EPS) ** 2)\n",
    "\n",
    "\n",
    "def mae(preds, labels):\n",
    "    return torch.mean(torch.abs(preds - labels))\n",
    "\n",
    "\n",
    "def mse(preds, labels):\n",
    "    return torch.mean((preds - labels) ** 2)\n",
    "\n",
    "def mape(preds, labels):\n",
    "    return torch.mean(torch.abs((preds - labels) / (labels + EPS)))\n",
    "\n",
    "def rmse(preds, labels):\n",
    "    return torch.sqrt(torch.mean((preds - labels) ** 2))\n",
    "\n",
    "\n",
    "def calc_metrics(preds, labels):\n",
    "    return rse(preds, labels), mae(preds, labels), mse(preds, labels), mape(preds, labels), rmse(preds, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger:\n",
    "\n",
    "    def __init__(self, logdir):\n",
    "        \n",
    "        print('---> logdir:', logdir)\n",
    "\n",
    "        os.makedirs(logdir, exist_ok=True)\n",
    "\n",
    "        self.writer = SummaryWriter(log_dir=logdir)\n",
    "        self.logdir = logdir\n",
    "\n",
    "        self.min_val_loss = np.inf\n",
    "        self.patience = 0\n",
    "        self.best_model_save_path = os.path.join(logdir, 'best_model.pth')\n",
    "\n",
    "        self.metrics = []\n",
    "        self.stop = False\n",
    "\n",
    "    def summary(self, m, model, epoch, patience):\n",
    "        m = pd.Series(m)\n",
    "        self.metrics.append(m)\n",
    "        if m.val_loss < self.min_val_loss:\n",
    "            torch.save(model.state_dict(), self.best_model_save_path)\n",
    "            self.patience = 0\n",
    "            self.min_val_loss = m.val_loss\n",
    "        else:\n",
    "            self.patience += 1\n",
    "        met_df = pd.DataFrame(self.metrics)\n",
    "        description = 'train loss: {:.5f} val_loss: {:.5f} | best val_loss: {:.5f} patience: {}'.format(\n",
    "            m.train_loss,\n",
    "            m.val_loss,\n",
    "            self.min_val_loss,\n",
    "            self.patience)\n",
    "\n",
    "        met_df.round(6).to_csv('{}/train_metrics.csv'.format(self.logdir))\n",
    "\n",
    "        self.writer.add_scalar('Loss/Train', m.train_loss, epoch)\n",
    "        self.writer.add_scalar('Loss/Val', m.val_loss, epoch)\n",
    "\n",
    "        if self.patience >= patience:\n",
    "            self.stop = True\n",
    "        return description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, optimizer, x, y):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    # input = torch.nn.functional.pad(input, (1, 0, 0, 0))\n",
    "\n",
    "    output = model(x)  # now, output = [bs, seq_y, n]\n",
    "    \n",
    "    predict = output\n",
    "\n",
    "    if len(predict.size()) != len(y.size()):\n",
    "        predict = torch.reshape(predict, y.shape)\n",
    "\n",
    "    loss = lossfn(predict, y)\n",
    "    rse, mae, mse, mape, rmse = calc_metrics(predict, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item(), rse.item(), mae.item(), mse.item(), mape.item(), rmse.item()\n",
    "\n",
    "\n",
    "def evaluating(model, x, y):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    output = model(x)  # now, output = [bs, seq_y, n]\n",
    "    predict = output\n",
    "    \n",
    "    predict = torch.clamp(predict, min=0., max=10e10)\n",
    "    \n",
    "\n",
    "    loss = lossfn(predict, y)\n",
    "    rse, mae, mse, mape, rmse = calc_metrics(predict, y)\n",
    "\n",
    "    return loss.item(), rse.item(), mae.item(), mse.item(), mape.item(), rmse.item()\n",
    "\n",
    "\n",
    "def testing(model, test_loader, out_seq_len):\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    y_real = []\n",
    "    for i, (x, y) in enumerate(test_loader):\n",
    "\n",
    "        preds = model(x)\n",
    "      \n",
    "        preds = preds\n",
    "\n",
    "        outputs.append(preds)\n",
    "        y_real.append(y)\n",
    "\n",
    "    yhat = torch.cat(outputs, dim=0)\n",
    "    y_real = torch.cat(y_real, dim=0)\n",
    "    test_met = []\n",
    "\n",
    "    yhat[yhat < 0.0] = 0.0\n",
    "\n",
    "    if len(yhat.size()) != len(y_real.size()):\n",
    "        yhat = torch.reshape(yhat, y_real.shape)\n",
    "\n",
    "    test_met.append([x.item() for x in calc_metrics(yhat, y_real)])\n",
    "    test_met_df = pd.DataFrame(test_met, columns=['rse', 'mae', 'mse', 'mape', 'rmse']).rename_axis('t')\n",
    "    return test_met_df, y_real, yhat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "input_dim = 144\n",
    "hidden_dim = 100\n",
    "lossfn = torch.nn.MSELoss()\n",
    "seq_length=[24, 12, 6, 3]\n",
    "runs = [1,2,3,4,5]\n",
    "\n",
    "\n",
    "#n_layer=2\n",
    "\n",
    "for seq_len in seq_length:\n",
    "    print('***seq_len***', seq_len)\n",
    "    for run in runs:\n",
    "        print('<--run-->', run)\n",
    "        model = LSTM_TM(input_dim, hidden_dim, seq_len=seq_len)\n",
    "        model.to(device)\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "        x_train, y_train = create_dataset(train_df, train_df, seq_len)\n",
    "        x_test, y_test = create_dataset(test_df, test_df, seq_len)\n",
    "        x_val, y_val = create_dataset(val_df, val_df, seq_len)\n",
    "\n",
    "        train_loader = TensorDataset(torch.from_numpy(x_train).float(),torch.from_numpy(y_train).float())   \n",
    "        val_loader = TensorDataset(torch.from_numpy(x_val).float(),torch.from_numpy(y_val).float())\n",
    "        test_loader = TensorDataset(torch.from_numpy(x_test).float(),torch.from_numpy(y_test).float())\n",
    "\n",
    "        train_loader = DataLoader(train_loader, batch_size=64, shuffle=False)\n",
    "        val_loader = DataLoader(val_loader, batch_size=64, shuffle=False)\n",
    "        test_loader = DataLoader(test_loader, batch_size=64, shuffle=False)\n",
    "        \n",
    "        parent_logs_path='logs'\n",
    "        logdir='lstm'\n",
    "        dataset='abilene'\n",
    "        logdir=logdir+'_data_{}_seq_{}'.format(dataset, seq_len)\n",
    "        _logger = os.path.join(parent_logs_path, logdir, 'run_{}'.format(run))\n",
    "        logger = Logger(logdir=_logger)\n",
    "        \n",
    "        print('|--- Training ---|')\n",
    "        epochs=500\n",
    "        #iterator = 10\n",
    "        iterator = trange(epochs)\n",
    "        tmps_train = time.time()\n",
    "\n",
    "        for epoch in iterator:\n",
    "            train_loss, train_rse, train_mae, train_mse, train_mape, train_rmse = [], [], [], [], [], []\n",
    "            for i, (x, y) in enumerate(train_loader):\n",
    "                # x -->  [b, seq_x, n, f]\n",
    "                # y --> [b, seq_y, n]\n",
    "\n",
    "                loss, t_rse, t_mae, t_mse, t_mape, t_rmse = training(model, optimizer, x, y)\n",
    "\n",
    "                train_loss.append(loss)\n",
    "                train_rse.append(t_rse)\n",
    "                train_mae.append(t_mae)\n",
    "                train_mse.append(t_mse)\n",
    "                train_mape.append(t_mape)\n",
    "                train_rmse.append(t_rmse)\n",
    "\n",
    "\n",
    "            with torch.no_grad():\n",
    "                val_loss, val_rse, val_mae, val_mse, val_mape, val_rmse = [], [], [], [], [], []\n",
    "                for i, (x, y) in enumerate(val_loader):\n",
    "\n",
    "                    metrics = evaluating(model, x, y)\n",
    "                    val_loss.append(metrics[0])\n",
    "                    val_rse.append(metrics[1])\n",
    "                    val_mae.append(metrics[2])\n",
    "                    val_mse.append(metrics[3])\n",
    "                    val_mape.append(metrics[4])\n",
    "                    val_rmse.append(metrics[5])\n",
    "\n",
    "            m = dict(train_loss=np.mean(train_loss), train_rse=np.mean(train_rse),\n",
    "                    train_mae=np.mean(train_mae), train_mse=np.mean(train_mse),\n",
    "                    train_mape=np.mean(train_mape), train_rmse=np.mean(train_rmse),\n",
    "                    val_loss=np.mean(val_loss), val_rse=np.mean(val_rse),\n",
    "                    val_mae=np.mean(val_mae), val_mse=np.mean(val_mse),\n",
    "                    val_mape=np.mean(val_mape), val_rmse=np.mean(val_rmse))\n",
    "\n",
    "            description = logger.summary(m, model, epoch, patience=50 )\n",
    "\n",
    "            if logger.stop:\n",
    "                break\n",
    "\n",
    "            description = 'Epoch: {} '.format(epoch) + description\n",
    "            iterator.set_description(description)\n",
    "\n",
    "        tmps_t2 = time.time() - tmps_train\n",
    "        print(\"Train_Temps = %f\" % tmps_t2)\n",
    "\n",
    "        model.load_state_dict(torch.load(logger.best_model_save_path))\n",
    "\n",
    "        print('|--- Testing ---|')\n",
    "        tmps_test = time.time()\n",
    "        with torch.no_grad():\n",
    "            test_met_df, y_real, yhat = testing(model, test_loader, out_seq_len=1)\n",
    "            tmps_test2 = time.time() - tmps_test\n",
    "            print(\"Test_Temps = %f\" % tmps_test2)\n",
    "        print('Test metric: ', test_met_df)\n",
    "        test_met_df.to_csv(os.path.join(logger.logdir, 'test_metrics.csv'))\n",
    "        np.save(os.path.join(logger.logdir, 'y_real_data'), y_real.detach().cpu().numpy())\n",
    "        np.save(os.path.join(logger.logdir, 'y_pred_data'), yhat.detach().cpu().numpy())\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cnn_lstm_model = Sequential([\n",
    "    #Conv1D(filters=32, \n",
    "           #kernel_size=(CONV_WIDTH,), \n",
    "           #activation='relu'),\n",
    "    #LSTM(32, return_sequences=True),\n",
    "    #LSTM(32, return_sequences=True),\n",
    "    #Dense(1)\n",
    "#])\n",
    "\n",
    "#history = compile_and_fit(cnn_lstm_model, conv_window)\n",
    "\n",
    "#val_performance['CNN+LSTM'] = cnn_lstm_model.evaluate(conv_window.val)\n",
    "#performance['CNN+LSTM'] = cnn_lstm_model.evaluate(conv_window.test, verbose=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
